<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="GENERATOR" content="Microsoft FrontPage 5.0">
   <title>... in Java by Richard G Baldwin</title>
</head>
<body link="#DD0000" vlink="#0000FF" alink="#FF0000" lang="EN-US">
<h2>
Java Sound, Getting Started, Part 1, Playback</h2>
<i>Baldwin shows you how to use the Sound API to play back previously captured 
audio data.</i><p><b>Published:</b>&nbsp; January 21, 2003<br><b>By <a href="mailto:Baldwin@DickBaldwin.com">Richard G. Baldwin</a></b>
<p>Java Programming Notes # 2008<ul >
<li>
<a href="#Preface">Preface</a></li>
<li>
<a href="#Preview">Preview</a></li>

<li>
<a href="#Discussion and Sample Programs">Discussion and Sample Code</a></li>

<li>
<a href="#Run the program">Run the Program</a></li>

<li>
<a href="#Summary">Summary</a></li>

<li>
<a href="#Whats Next">What's Next?</a></li>

<li>
<a href="#Complete Program Listings">Complete Program Listing</a></li>
</ul>

<hr size=3 width="100%" align=center>
<center>
<h2>
<a NAME="Preface"></a>Preface</h2></center>
<p>
This is the second lesson in a series of lessons designed to teach you how to use the 
Java Sound API.&nbsp; The first lesson in the series was entitled 
<a href="http://www.developer.com/java/other/article.php/1565671">Java Sound, An 
Introduction</a>.</p>
<p>
<font color="#FF0000"><b>What is sound?</b></font></p>
<p>From a human perspective, sound is the sensation that we experience when 
pressure waves impinge upon the small parts contained inside our ears.<p>The ultimate purpose of 
the Sound API is to assist you in  writing  programs that will cause specific sound pressure 
waves to impinge upon the ears of targeted individuals 
at specific times.<p><font color="#FF0000"><b>Two types of sound</b></font></p>
<p>
Two significantly different 
types of audio <i>(or sound)</i> data are supported by the Java Sound API:<ul>
  <li>Sampled audio data</li>
  <li>Musical Instrument Digital Interface (MIDI) data</li>
  </ul>
  <p>I explained the difference between the two types of audio data in the 
  previous lesson.&nbsp; Because the two types of audio data are so different, I 
  am concentrating on sampled audio data at this point in time.&nbsp; I will defer any detailed discussion of 
      MIDI  data until later.<p>
<b><font color="#FF0000">Viewing tip</font></b>
<p>You may find it useful to open another copy of this lesson in a separate
browser window.&nbsp; That will make it easier for you to scroll back and
forth among the different listings and figures while you are reading about
them.
<p><b><font color="#FF0000">Supplementary material</font></b>
<p>I recommend that you also study the other lessons in my extensive collection
of online Java tutorials.&nbsp; You will find those lessons published at
<a href="http://softwaredev.earthweb.com/java">Gamelan.com</a>.&nbsp;
However, as of the date of this writing, Gamelan doesn't maintain a consolidated
index of my Java tutorial lessons, and sometimes they are difficult to
locate there.&nbsp; You will find a consolidated index at <font color="#000000">
<a href="http://www.DickBaldwin.com">www.DickBaldwin.com</a>.</font>
  <h2 align="center"><font color="#000000"><a name="Preview">Preview</a></font></h2>
<p>The Java Sound API is based on the concept of <i>lines</i> and <i>mixers.</i><p>
I will provide a description of the physical and electrical characteristics of 
sound, in preparation for the introduction of an<i> audio mixer.</i><p>I will 
use the scenario of a rock concert with six microphones and two stereo speakers 
to describe one of the ways that audio mixers are used.<p>I will discuss a 
variety of Java Sound programming topics, including mixers, lines, data format, 
etc.<p>I will explain the general relationship that exists among <b>
SourceDataLine</b> objects, <b>Clip</b> objects, <b>Mixer</b> objects, <b>
AudioFormat</b> objects, and ports in a simple audio output program.<p>I will 
provide a 
program that you can use to first capture and then to play back audio sound.<p>I 
will provide a detailed explanation of the code used to play back the audio data 
captured in memory by this program.<p>I will defer a detailed explanation of the 
code used to capture the audio data until the next lesson in the series.<center>
<h2>
<a NAME="Discussion and Sample Programs"></a><font color="#000000">Discussion
and Sample Code</font></h2></center>
<p><font color="#FF0000"><b>Physical and electrical characteristics of sound</b></font><p>The purpose of this lesson is to 
get you started writing programs using the Java 
Sound API.<p>The Sound API is based on the concept of an audio <i>mixer, </i>
which<i>&nbsp;</i>is a device commonly used in the production of sound for 
concerts, music CDs, etc.&nbsp; Before getting into the details of an audio mixer, it will be useful to review 
the physical and electrical characteristics of sound.&nbsp; Consider the picture 
shown in Figure 1.<p align="center">
<img border="0" src="java2008a.gif" width="400" height="300"><p align="center">
Figure 1 Joe Politico making a speech<p><font color="#FF0000"><b>Joe Politico 
making a speech</b></font><p>Figure 1 shows a picture of a 
politician named Joe Politico, making a speech using a system commonly known as 
a PA or <i>public address</i> system.&nbsp; A PA system typically consists of a 
microphone, an amplifier, and a loud speaker.&nbsp; The purpose of the PA system 
is to amplify Joe's voice so that it can be heard in the back of the crowd.<p>
<font color="#FF0000"><b>Vibrations in the air</b></font><p>
Briefly, when Joe speaks, his vocal cords cause the air particles in his throat to vibrate.&nbsp; 
This, in turn causes sound pressure waves to impinge on a microphone, where the 
vibrations are converted to very low level electrical waves that mimic the sound 
pressure waves.&nbsp; The electrical waves are fed into an amplifier that 
amplifies the electrical waves.&nbsp; From there, the electrical waves are fed 
into a loud speaker, which converts the amplified electrical waves back into 
high-intensity sound pressure waves that mimic the original sound pressure waves 
created by Joe's vocal cords.<p><font color="#FF0000"><b>A dynamic microphone</b></font><p>Now consider Figure 2, which shows a schematic 
diagram of a type of microphone known as a dynamic microphone.<p align="center">
<img border="0" src="java2008b.gif" width="400" height="300"><p align="center">
Figure 2 Schematic of a dynamic microphone<p><font color="#FF0000"><b>Sound 
pressure waves impinge on a diaphragm</b></font><p>Sound pressure waves created by 
Joe's vocal cords impinge on a flexible diaphragm inside the microphone.&nbsp; 
The sound pressure waves cause the diaphragm to vibrate, and the vibrations of 
the diaphragm mimic the sound pressure waves.<p><font color="#FF0000"><b>A coil 
of wire is caused to move</b></font><p>A coil made of very fine 
wire is attached to the diaphragm.&nbsp; As the diaphragm vibrates, the coil is 
caused to move back and forth in a magnetic field caused by a strong permanent 
magnet.&nbsp; It is a well-known fact that when a coil of wire cuts through a 
magnetic field, a voltage is induced in the coil.<blockquote>
  <p><i>(In fact, your local electrical power station generates power by 
  spinning a huge coil in a very strong magnetic field.&nbsp; This results in 
  the voltage that you use to drive the appliances in your house.)</i></blockquote>
<p><font color="#FF0000"><b>Electrical signal mimics Joe's sound pressure waves</b></font><p>Thus, a very weak voltage is induced in the coil in the microphone, and the 
waveform of the voltage mimics the sound pressure waves that impinge on the 
diaphragm.&nbsp; This is the voltage that is fed into the amplifier in Figure 1.<p>
<font color="#FF0000"><b>A loud speaker</b></font><p>
As it turns out, the principle behind a loud speaker is simply a dynamic microphone operated in the 
reverse direction, as shown in Figure 3.&nbsp; <i>(Obviously, the wires are 
bigger and the diaphragm is larger in order to handle the amplified signals.)</i><p align="center">
<img border="0" src="java2008c.gif" width="400" height="300"><p align="center">
Figure 3 Schematic diagram of a loud speaker<p>In Figure 3, the amplified 
electrical signal, whose waveform mimics Joe's sound pressure waves, is applied to 
the moving coil on the right side of the figure.&nbsp; This moving coil floats in a strong 
magnetic field and is attached to a large flexible diaphragm.<p>
<font color="#FF0000"><b>Another well-known fact</b></font><p>It is a 
well-know fact that when you pass a current through a coil of wire, a magnetic 
field is induced.&nbsp; The interaction of the magnetic field produced by the 
coil and the  magnetic field created by the permanent magnet causes the coil to 
move back and forth.&nbsp; This causes the diaphragm to vibrate.<p>
<font color="#FF0000"><b>Another vibrating diaphragm</b></font><p>The 
vibrations of the diaphragm in the loud speaker cause the surrounding air 
particles to vibrate and to create high-intensity sound 
pressure waves.&nbsp; The waveform of these high-intensity sound pressure waves 
mimic the low-intensity sound pressure waves created by Joe's vocal cords.&nbsp; 
The new sound pressure waves 
are strong enough to impinge upon the ears of the persons in the back of the 
crowd.&nbsp; Thus, Joe's amplified voice can be heard in the back of the crowd.<p>
<font color="#FF0000"><b>A rock concert</b></font><p>
By now, you might be asking what this has to do with the Java Sound API.&nbsp; 
This is my lead-in to the concept of an audio mixer.<p>The scenario discussed 
above is fairly simple.&nbsp; It consists of one set of vocal cords, one 
microphone, one amplifier, and one loud speaker.&nbsp; Now consider the scenario 
depicted in Figure 4, 
which shows the stage being 
prepared for a rock concert.<p align="center">
<img border="0" src="java2008d.gif" width="400" height="300"><p align="center">
Figure 4 The stage for a rock concert<p><font color="#FF0000"><b>Six microphones 
and two speakers</b></font><p>In Figure 4, six separate microphones 
have been placed on the stage.&nbsp; Two loud speakers are placed on 
either side of the stage.&nbsp; When the concert begins, different performers 
will sing and play music into each of the six microphones.&nbsp; Each of the 
microphones will produce electrical signals, which must be individually 
amplified and fed into the two loud speakers.&nbsp; In addition, special sound 
effects, such as reverberation may be applied to the electrical signals before 
they are fed into the loud speakers.<p><font color="#FF0000"><b>A stereo 
experience</b></font><p>The two loud speakers are 
intended to provide a stereo audio experience.&nbsp; For example, all of the 
electrical signal produced by the right-most microphone might be applied only to 
the speaker on the right.&nbsp; Similarly, all of the electrical signal produced by the 
left-most microphone might be applied only to the speaker on the left.&nbsp; The 
electrical signals produced by the other microphones might be applied to the two 
speakers in proportion to their positions from left to right between the 
speakers.&nbsp; The 
electrical signals from the two microphones in the center might be applied equally 
to the two speakers.<p><font color="#FF0000"><b>An audio mixer</b></font><p>This task is often accomplished with an electronic device 
referred to as an <i>audio mixer.</i><blockquote>
  <p><i>(If you enter the keywords <b>audio mixer concert -mp3</b> into the
  <a href="http://www.google.com/">Google</a> search engine, you can view 
  material on hundreds of actual audio mixers. As of the date of this writing, you can 
  read the specifications for, and view a picture of one such audio mixer 
  (chosen at random) at the following
  <a href="http://www.crmav.com/recording/gear/98/sr32_4_audio_mixer.shtml">URL</a>.)</i></blockquote>
<p><font color="#FF0000"><b>An audio line</b></font><p>Although I am no expert on audio mixers, it is my understanding that a 
typical mixer has the ability to accept many independent electrical signals, 
each of which represents an independent audio signal or <i>line</i>.<blockquote>
  <p><i>(The concept of an audio <b>line</b> will become very important later when we get into the 
  details of the Java Sound API.</i><p><i>Perhaps this terminology derives from the 
  fact that engineers and technicians often refer to the wires that carry 
  electrical signals as lines.&nbsp; For example, I have two telephone lines in 
  my house.&nbsp; We might also say that you should avoid high-voltage power lines.&nbsp; Also perhaps, the terminology derives from the fact that the 
  controls for each audio signal in a mixer are often arranged in a line, as in 
  the picture of the audio mixer referred to above.)</i></blockquote>
<p><font color="#FF0000"><b>Process each audio line independently</b></font><p>In any event, a typical 
audio mixer has the ability to apply amplification to each 
audio line independently of the amplification being applied to the other audio 
lines.&nbsp; The mixer may also have the ability to apply special audio effects, 
such as reverberation, to each of the lines.&nbsp; Finally, the mixer will have 
the ability to mix the individual amplified signals into output lines and to control the 
contribution that each input line makes to each output line.&nbsp; <i>
(Controlling the contribution of each input line to each output line is often 
referred to as pan.)</i><p><font color="#FF0000"><b>Back to the 
stereo experience</b></font><p>Thus, in the scenario pictured in Figure 
4, the mixer operator has the ability to add together the signals produced by 
the six microphones in order to produce two output signals, each of which will 
be applied to one of the stereo loud speakers.<p>In effect, the signals from 
the six microphones can be added together to produce two output signals with the 
contribution of each microphone to each output signal being based on the physical position of the 
microphone on the stage.&nbsp; <i>(By changing the pan, a good mixer operator 
could even change that contribution over time as the lead singer moves back and 
forth across the stage.)</i>&nbsp; By controlling things in this way, the mixer 
operator can produce a stereo audio experience for the people in the audience.<p>
<font color="#FF0000"><b>Now to the programming world</b></font><p>
Let's shift the discussion from the physical world to the programming world.&nbsp;
According to Sun,<blockquote>
  <p><i>&quot;Java Sound does not assume a specific audio hardware configuration; it 
  is designed to allow different sorts of audio components to be installed on a 
  system and accessed by the API. Java Sound supports common functionality such 
  as input and output from a sound card (for example, for recording and playback 
  of sound files) <b>as well as mixing of multiple streams of audio</b>.&quot;</i></blockquote>
<p><font color="#FF0000"><b>Mixers and Lines</b></font><p>Fundamentally, the sound API is 
constructed around the concept of <i>mixers</i> and <i>lines</i>.&nbsp; Moving from the physical world 
discussed above to the programming world, this is part of what Sun has to say 
about a mixer:<blockquote>
  <p><i>&quot;A mixer is an audio device with one or more lines. It need not be 
  designed for mixing audio signals. A mixer that actually mixes audio has 
  multiple input (source) lines and at least one output (target) line.</i><p><i>The 
  former are often instances of classes that implement <b>SourceDataLine</b>, 
  and the latter, <b>TargetDataLine</b>. <b>Port</b> objects, too, are either 
  source lines or target lines. A mixer can accept prerecorded, loopable sound 
  as input, by having some of its source lines be instances of objects that 
  implement the <b>Clip</b> interface.&quot;</i></blockquote>
<p><font color="#FF0000"><b>The Line interface</b></font><p>Sun has this to say about the <b>Line</b> interface:<blockquote>
  <p><i>&quot;A line is an element of the digital audio &quot;pipeline,&quot; such as an audio 
  input or output port, a mixer, or an audio data path into or out of a mixer. 
  The audio data flowing through a line can be mono or multichannel (for 
  example, stereo). ... A line can have <b>controls</b>, such as gain, pan, and 
  reverb.&quot;</i></blockquote>
<p><font color="#FF0000"><b>Putting the terms together</b></font><p>The earlier quotation from Sun mentioned the following terms:<ul>
  <li>SourceDataLine</li>
  <li>TargetDataLine</li>
  <li>Port</li>
  <li>Clip</li>
  <li>Controls</li>
  </ul>
<p>Figure 5 shows how four of these terms can come together to form a simple 
audio output program.&nbsp; <i>(I will discuss a simple audio input program in the 
next lesson.)</i><p align="center">
  <img border="0" src="java2008e.gif" width="401" height="400"><p align="center">
  Figure 5 An audio output system<p>
<font color="#FF0000"><b>The programming scenario</b></font><p>
From a programming viewpoint, Figure 5 indicates that a <b>Mixer</b> object has been obtained with one <b>Clip</b> object 
and two <b>SourceDataLine</b> objects.&nbsp;  <p>
<font color="#FF0000"><b>What is a Clip?</b></font><p>
A <b>Clip</b> is a  mixer 
input object whose content doesn't change with time.&nbsp; In other words, you 
load the audio data into the <b>Clip</b> object prior to playback.&nbsp; The 
audio contents of the <b>Clip</b> can then be played back one or more times.&nbsp; 
You can cause the <b>Clip</b> to loop and continue playing the same audio data 
over and over.<p>
<font color="#FF0000"><b>A streaming input</b></font><p>
A <b>SourceDataLine</b> object, on the other hand, is a <i>streaming</i> mixer 
input object. An object of this type can accept a stream of audio data 
and push that audio data into the mixer in real time.&nbsp; The 
actual audio data can derive from a variety of sources, such as an audio file, a 
network connection, or a buffer in memory.<p>
<font color="#FF0000"><b>Different kinds of lines</b></font><p>
Thus, <b>Clip</b> objects and <b>SourceDataLine</b> objects can be viewed as 
input lines to a <b>Mixer</b> object.&nbsp; Each of these input lines can have 
its own <i>reverberation, gain, </i>and<i> pan</i> controls.<p>
As indicated in Figure 5, the <b>Mixer</b> object can also include 
reverberation, gain, and pan controls.<p>
<font color="#FF0000"><b>Playing back audio</b></font><p>
In this simple system, the <b>Mixer</b> reads data from the input lines, uses 
the controls to mix the input signals, and delivers the final output to one or 
more output ports, such as a speaker, a headphone jack, etc.<p>
In the earlier lesson entitled <i>Java Sound, An Introduction,</i> I provided a simple program that captures audio data from 
a microphone port, stores that data in memory, and plays the captured data back 
through a speaker port.&nbsp; A copy of that program is provided in Listing 11 
near the end of this lesson.<p>
<font color="#FF0000"><b>Will discuss capture and playback</b></font><p>
A large portion of this program is dedicated to creating a graphical user 
interface that is used to control the operation of the program.&nbsp; I won't discuss 
those parts of the program.&nbsp; However, I will provide a  discussion of 
the capture and playback portions of the program.&nbsp; I will discuss the 
playback portion in this lesson, and will discuss the capture portion in the 
next lesson.&nbsp; During this discussion, I will illustrate the use of audio 
lines with the Java Sound API.<p>
<font color="#FF0000"><b>Captured data stored in a ByteArrayOutputStream object</b></font><p>
The capture portion of the program captures audio data 
from the microphone and stores it in a <b>ByteArrayOutputStream</b> object.&nbsp; 
 
<p>
The playback method named <b>playAudio</b>, which begins in Listing 1, plays back 
the audio data that has been captured and saved in the <b>ByteArrayOutputStream</b> object.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>  private void <b>playAudio</b>() {
    try{
      byte audioData[] =
                 byteArrayOutputStream.
                         toByteArray();

      InputStream byteArrayInputStream
            = new ByteArrayInputStream(
                            audioData);

<b><font face="Courier New,Courier">Listing 1</font></b></pre>
</td>
</tr>
</table>

<p>
<font color="#FF0000"><b>Begin with plain-vanilla code</b></font><p>
The code fragment in Listing 1 really doesn't have anything in particular to do with Java 
Sound.&nbsp; Rather, this is plain-vanilla code whose purpose is to:<ul>
  <li>Convert the previously saved data into an array of type <b>byte</b>.</li>
  <li>Get an input stream on the array of <b>byte</b> data.</li>
  </ul>

<p>
This is necessary to make the audio data available for playback<p>
<font color="#FF0000"><b>Now for the Sound API</b></font>.<p>
The code in Listing 2 is very particular to the Java Sound API.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>      AudioFormat audioFormat =
                      getAudioFormat();

<b><font face="Courier New,Courier">Listing 2</font></b></pre>
</td>
</tr>
</table>
<p>
   <p>
   <p>
At this point, I will briefly touch on a topic that I plan to explain in much 
more detail in a subsequent lesson.<p>
<font color="#FF0000"><b>Two independent formats</b></font><p>
Frequently, there are two  independent formats  involved in audio data:<ul>
    <li>The format of the file <i>(if any)</i> that contains the audio data <i>
    (there is no file involved in this program because the audio data is saved 
    in memory).</i></li>
    <li>The format of the sampled audio data itself.</li>
    </ul>
   <p>
<font color="#FF0000"><b>What is the audio format?</b></font><p>
Here is part of what Sun has to say about audio format:<blockquote>
   <p>
<i>&quot;Each data line has an audio format associated with its data stream. The 
format (an instance of <b>AudioFormat</b>) specifies the arrangement of the bytes in 
the audio stream. Some of the format's properties are the number of channels, 
the sample rate, the sample size, and the encoding technique. Common encoding 
techniques include linear pulse-code modulation (PCM), mu-law encoding, and 
a-law encoding.&quot;</i></blockquote>
   <p>
<font color="#FF0000"><b>A sequence of bytes</b></font><p>
The sampled audio data consists of a sequence of bytes.&nbsp; There are a 
variety of optional ways that these bytes can be arranged and interpreted.&nbsp; I won't go 
into  detail at this point regarding the many options.&nbsp; However, I will 
briefly discuss the format that is used for the audio data in this program.<p>
<font color="#FF0000"><b>A short side trip</b></font><p>
At this point, I will depart from the method named <b>playAudio</b> and discuss 
the method named <b>getAudioFormat</b> <i>(that is called in Listing 2).</i>&nbsp; The 
entire method named <b>getAudioFormat</b> is shown in Listing 3.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>  private AudioFormat <b>getAudioFormat</b>(){
    float sampleRate = 8000.0F;
    int sampleSizeInBits = 16;
    int channels = 1;
    boolean signed = true;
    boolean bigEndian = false;
    return new AudioFormat(
                      sampleRate,
                      sampleSizeInBits,
                      channels,
                      signed,
                      bigEndian);
  }//end getAudioFormat

<b><font face="Courier New,Courier">Listing 3</font></b></pre>
</td>
</tr>
</table>
<p>
   <p>
   <p>
Aside from some initialized variable declarations, the code in Listing 3 
consists of a single executable statement.<p>
<font color="#FF0000"><b>An AudioFormat object</b></font><p>
The<b> getAudioFormat</b> method creates and returns an object of the<b> 
AudioFormat</b> class.&nbsp; Here is part of what Sun has to say about this 
class:<blockquote>
   <p>
<i>&quot;<b>AudioFormat</b> is the class that specifies a particular 
arrangement of data in a sound stream. By examining the information stored in 
the audio format, you can discover how to interpret the bits in the binary sound 
data.&quot;</i></blockquote>
   <p>
<font color="#FF0000"><b>Used the simplest constructor</b></font><p>
The <b>AudioFormat</b> class has two constructors.&nbsp; <i>(I elected to use the 
simpler of the two.)</i>&nbsp; For this constructor, the required parameters are:<ul>
      <li>Sample rate in samples per second.&nbsp; <i>(Allowable values include 
      8000, 11025, 16000, 22050, and 44100 samples per second.)</i></li>
      <li>Sample size in bits.&nbsp; <i>(Allowable values include 8 and 16 bits per 
      sample.)</i></li>
      <li>Number of channels.&nbsp; <i>(Allowable values include 1 channel for mono 
      and 2 channels for stereo.)</i></li>
      <li>Signed or unsigned data.&nbsp; <i>(Allowable values include true and false 
      for signed data or unsigned data.)</i></li>
      <li>Big-endian or little-endian order.&nbsp; <i>(This has to do with the order 
      in which the data bytes are stored in memory.&nbsp; You can learn about 
      this topic <a href="http://mindprod.com/jglossendian.html">here</a>.)</i></li>
      </ul>
   <p>
As you can see in Listing 3, this method specifies the following parameters for 
the new <b>AudioFormat</b> object:<ul>
        <li>8000 samples per second</li>
        <li>16 bits per sample</li>
        <li>1 channel <i>(mono)</i></li>
        <li>Signed data</li>
        <li>Little-endian order</li>
        </ul>
   <p>
<font color="#FF0000"><b>Default data encoding is linear PCM</b></font><p>
The constructor that I used constructs an <b>AudioFormat</b> object with a 
linear PCM encoding and the parameters listed above <i>(I will have more to say 
about linear PCM encoding and other encoding schemes in future lessons).</i><p>
<font color="#FF0000"><b>Now back to the playAudio method</b></font><p>
Now that we understand something about the format of our audio data, let's return 
our attention to the method named <b>playAudio</b>.&nbsp; When we actually play the audio 
data later, we will need an object of the class named <b>AudioInputStream</b>.&nbsp; 
We instantiate such an object in Listing 4.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>      audioInputStream =
        new AudioInputStream(
          byteArrayInputStream,
          audioFormat,
          audioData.length/audioFormat.
                       getFrameSize());

<b><font face="Courier New,Courier">Listing 4</font></b></pre>
</td>
</tr>
</table>
<p><font color="#FF0000"><b>Parameters for AudioInputStream constructor</b></font></p>
<p>The constructor for the <b>AudioInputStream</b> class requires the following 
three parameters:</p>
<ul>
  <li>The stream on which this <b>AudioInputStream</b> object is based.&nbsp; <i>
  (As 
  you can see, it is based on the <b>ByteArrayInputStream</b> object that was 
  created earlier.)</i></li>
  <li>The format of this stream's audio data.&nbsp; <i>(For this, we use the <b>
  AudioFormat</b> object created earlier.)</i></li>
  <li>The length in sample frames of the data in this stream.&nbsp; <i>(See 
  explanation below.)</i></li>
</ul>
<p>The first two parameters should be obvious from the code in Listing 4.&nbsp; 
However, the third parameter isn't quite as obvious.</p>
<p><font color="#FF0000"><b>Getting the length in sample frames</b></font></p>
<p>As you can see in Listing 4, the third parameter value is created by doing 
some arithmetic.&nbsp; One of the attributes of the audio format that I didn't 
mention earlier is something called a <i>frame.</i>&nbsp;  </p>
<p><font color="#FF0000"><b>What is a frame?</b></font></p>
<p>For the simple linear 
PCM encoding scheme used in this program, a frame consists of the set of samples 
for all channels at a given point in time.</p>
<p>Therefore, the size of a frame
<i>(in bytes)</i> is equal to the size of a sample <i>(in bytes)</i> multiplied 
by the 
number of channels.</p>
<p>As you have probably already guessed, the method named
<b>getFrameSize</b> returns the frame size in bytes.</p>
<p><font color="#FF0000"><b>Calculating the length in sample frames</b></font></p>
<p>Therefore, the length 
of the audio data in sample frames can be calculated by dividing the total 
number of bytes in the audio data by the number of bytes in each sample frame.&nbsp; 
This is the calculation used for the third parameter in Listing 4.</p>
<p><font color="#FF0000"><b>Getting a SourceDataLine object</b></font></p>
<p>The portion of the program that we are discussing is a simple audio output 
system.&nbsp; As you may have surmised from the block diagram in Figure 5, we 
will need a <b>SourceDataLine</b> object to accomplish this task.</p>
<p>There 
are several ways to get a <b>SourceDataLine</b> object, none of which are particularly 
straightforward.&nbsp; The code in Listing 5 gets and saves a reference to an 
object of type <b>SourceDataLine</b>.</p>
<blockquote>
<p> <i>(Note that this code does not 
simply instantiate an object of the class <b>SourceDataLine</b>.&nbsp; Rather, 
it gets the object in a more roundabout fashion.)</i></p>
</blockquote>
<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>      DataLine.Info dataLineInfo =
                new DataLine.Info(
                  SourceDataLine.class,
                          audioFormat);

      sourceDataLine = (SourceDataLine)
                   AudioSystem.getLine(
                         dataLineInfo);

<b><font face="Courier New,Courier">Listing 5</font></b></pre>
</td>
</tr>
</table>
<p>
   <p>
   <p><font color="#FF0000"><b>What is a SourceDataLine object?</b></font></p>
<p>Here is part of what Sun has to say about the<b> SourceDataLine </b>type:</p>
<blockquote>
<p><i>&quot;A source data line is a data line to which data may be written. It acts 
as a source to its mixer. An application writes audio bytes to a source data 
line, which handles the buffering of the bytes and delivers them to the mixer. 
The mixer may ... deliver the mix to a target such as an output port ...</i></p>
  <p><i>Note that the naming convention for this interface reflects the 
  relationship between the line and its mixer ...&quot;</i></p>
</blockquote>
<p><font color="#FF0000"><b>The getLine method of the AudioSystem class</b></font></p>
<p>One of the ways of getting a <b>SourceDataLine</b> object is by invoking the 
static <b>getLine</b> method of the <b>AudioSystem</b> class <i>(I will have a 
lot more to say about the AudioSystem class in future lessons).</i></p>
<p>The <b>
getLine</b> method requires an incoming parameter of type <b>Line.Info</b>, and 
returns a <b>Line</b> object that matches the description in the specified <b>Line.Info</b> 
object.</p>
<p>
<font color="#FF0000"><b>Another short side trip</b></font><p>
Sun has this to say about the <b>Line.Info</b> object:<blockquote>
   <p>
<i>&quot;A line has an information object (an instance of Line.Info) that indicates 
what mixer (if any) sends its mixed audio data as output directly to the line, 
and what mixer (if any) gets audio data as input directly from the line. 
Subinterfaces of Line may have corresponding subclasses of Line.Info that 
provide other kinds of information specific to the particular types of line.&quot;</i></blockquote>
   <p>
<font color="#FF0000"><b>A DataLine.Info object</b></font><p>
The first statement in Listing 5 creates a new <b>DataLine.Info</b> object, 
which is a specialized <i>(subclass)</i> form of a <b>Line.Info</b> object.<p>
There are 
several overloaded constructors for the <b>DataLine.Info</b> class.&nbsp; I elected to use 
the simplest one.&nbsp; This constructor requires two parameters.<p>
<font color="#FF0000"><b>A Class object</b></font><p>
The 
first parameter is a <b>Class</b> object that represents the class of the data 
line described by the info object.&nbsp; As you can see, I specified this 
parameter as <b>SourceDataLine.class</b>.<p>
<font color="#FF0000"><b>The second parameter</b></font><p>
The second parameter specifies the desired audio data format for the line.&nbsp; 
I provided the <b>AudioFormat</b> object created earlier for this parameter.<p>
<font color="#FF0000"><b>Are we there yet?</b></font><p>
At this point, we still don't  have the required <b>SourceDataLine</b> object.&nbsp; 
All we have so far is an object that provides information about the required <b>
SourceDataLine</b> object.<p>
<font color="#FF0000"><b>Getting the SourceDataLine object</b></font><p>
The second statement in Listing 5  creates and saves the required <b>
SourceDataLine</b> object, by invoking the static <b>getLine</b> method of the <b>
AudioSystem</b> class, and passing the info object as a parameter.<blockquote>
  <p>
<i>(In the next lesson, I will show you how to get such a <b>Line</b> object by 
operating directly on a <b>Mixer</b> object.)</i></blockquote>
<p>
The <b>getLine</b> method 
returns a reference to the object as type <b>Line</b>, which is a superinterface 
of <b>SourceDataLine</b>.&nbsp; Therefore, a downcast is required before the 
return value can 
be saved as type <b>SourceDataLine</b>.<p>
<font color="#FF0000"><b>Preparing the SourceDataLine object for use</b></font><p>
Once we have the <b>SourceDataLine</b> object, we must prepare it for use by 
<i>opening</i> and <i>starting</i> it, as shown in Listing 6.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>      sourceDataLine.<b>open</b>(audioFormat);
      sourceDataLine.<b>start</b>();

<b><font face="Courier New,Courier">Listing 6</font></b></pre>
</td>
</tr>
</table>
<p>
   <p>
   <p>
<font color="#FF0000"><b>The open method</b></font><p>
As you can see in Listing 6, I passed the <b>AudioFormat</b> object to the <b>
open</b> method of the <b>SourceDataLine</b> object.&nbsp; According to Sun, 
this method:<blockquote>
  <p>
<i>&quot;Opens the line with the specified format, causing the line to acquire any 
  required system resources and become operational.&quot;</i></blockquote>
<p>
<font color="#FF0000"><b>Open status</b></font><p>
Here is a little more of what Sun has to say on this topic:<blockquote>
  <p>
<i>&quot;Opening and closing a line affects its resource allocation. Successful 
opening of a line guarantees that resources have been allocated to the line.</i><p>
<i>Opening a mixer that has audio input and/or output ports normally involves 
acquiring the native platform hardware resource (sound card) and initializing 
any required software components.</i><p>
<i>Opening a line that is a data path in or out 
of the mixer might involve device initialization as well as allocation of 
limited resources from the mixer. In other words, a mixer has a finite number of 
lines, so at some point multiple applications (or the same application) might 
vie for usage of the mixer.&quot;</i></blockquote>
<p>
<font color="#FF0000"><b>Invoking the start method on a line</b></font><p>
According to Sun, invoking the <b>start</b> method on a line:<blockquote>
  <p>
<i>&quot;Allows a line to engage in data I/O. If invoked on a line that is already 
running, this method does nothing. Unless the data in the buffer has been 
flushed, the line resumes I/O starting with the first frame that was unprocessed 
at the time the line was stopped.&quot;</i></blockquote>
<p>
In our case, of course, the line was never stopped.&nbsp; This was the first 
time that it was started.<p>
<font color="#FF0000"><b>Now we have most of what we need</b></font><p>
At this point, we have all the audio resources that we need to play back the audio 
data that was previously captured and saved in the <b>ByteArrayOutputStream</b> object 
<i>(recall that this object exists only in 
memory).</i><p>
<font color="#FF0000"><b>Going multithreaded</b></font><p>
We will create and start a thread running to accomplish the actual 
playback.&nbsp; The code in Listing 7 creates the thread and starts it running.<blockquote>
  <p>
<i>(Don't confuse the invocation of the start method on the thread with the 
invocation of the start method on the <b>SourceDataLine</b> object in Listing 
6.&nbsp; Those are entirely different operations.)</i></blockquote>
<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>      Thread playThread =
          new Thread(new <b>PlayThread</b>());
      playThread.start();
    } catch (Exception e) {
      System.out.println(e);
      System.exit(0);
    }//end catch
  }//end playAudio

<b><font face="Courier New,Courier">Listing 7</font></b></pre>
</td>
</tr>
</table>
  <p>
<font color="#FF0000"><b>Straightforward code</b></font><p>
The code in Listing 7 is straightforward, provided you understand multi-threaded 
programming in Java.&nbsp; If not, you can learn about multi-threaded 
programming by reviewing the tutorial lessons on the topic that I have published on my web site.<p>
Once the thread has been started, it will run until all of the 
previously-captured data has been played back.<p>
<font color="#FF0000"><b>A new Thread object</b></font><p>
The code in Listing 7 instantiates a new <b>Thread</b> object based on the class 
named <b>PlayThread</b>.&nbsp; The class named <b>PlayThread</b> is defined as 
an inner class in this program.&nbsp; The definition of the <b>PlayThread</b> 
class begins in Listing 8.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>class PlayThread extends Thread{
  byte tempBuffer[] = new byte[10000];

<b><font face="Courier New,Courier">Listing 8</font></b></pre>
</td>
</tr>
</table>
  <p>
<font color="#FF0000"><b>The run method of the Thread class</b></font><p>
Except for the declaration of an instance variable named <b>tempBuffer</b>, <i>(which 
is a <b>byte</b> array of size 10000 bytes),</i> the entire class definition is the 
definition of the required <b>run</b> method.&nbsp; As you  already know, 
invoking the <b>start</b> method on a <b>Thread</b> object causes that object's
<b>run</b> method to be executed.&nbsp; 
  <p>
The <b>run</b> method for this thread 
object begins in Listing 9.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>  public void <b>run</b>(){
    try{
      int cnt;
      //Keep looping until the input
      // read method returns -1 for
      // empty stream.
      while((cnt = audioInputStream.
        <b>read</b>(tempBuffer, 0,
            tempBuffer.length)) != -1){
        if(cnt > 0){
          //Write data to the internal
          // buffer of the data line
          // where it will be delivered
          // to the speaker.
          sourceDataLine.<b>write</b>(
                   tempBuffer, 0, cnt);
        }//end if
      }//end while

<b><font face="Courier New,Courier">Listing 9</font></b></pre>
</td>
</tr>
</table>
  <p>
<font color="#FF0000"><b>The first section of code in the run method</b></font><p>
The <b>run</b> method consists of two major sections, the first of which is 
shown in its entirety in Listing 9.<p>
In summary, a <b>while</b> loop is 
used to <b>read</b> audio data from the <b>AudioInputStream</b> object and to <b>
write</b> that data to the <b>SourceDataLine</b> object.<p>
Data written to the <b>SourceDataLine</b> object is automatically delivered to 
the speaker on the computer.&nbsp; A count value named <b>cnt</b> and a data 
buffer named<b> tempBuffer</b> are<b> </b>used to control the flow of data 
between the <b>read</b> and <b>write</b> operations.<p>
<font color="#FF0000"><b>The read method of AudioInputStream</b></font><p>
The <b>read</b> method of the <b>AudioInputStream</b> reads up to a specified 
maximum number of bytes of data from the <b>AudioInputStream</b>, putting them 
into the specified byte array, beginning at the specified byte index, <i>(which 
in this case is zero).</i>&nbsp;  <p>
<font color="#FF0000"><b>The return value</b></font><p>
The read method returns the total number of 
bytes read into the buffer, or -1 if there is no more data because the end of 
the stream has been reached.&nbsp; The number of bytes read into the buffer is 
saved in the variable named <b>cnt</b>.<p>
<font color="#FF0000"><b>The write method of SourceDataLine</b></font><p>
If the number of bytes read is greater than zero, the <b>write</b> method of the
<b>SourceDataLine</b> object is invoked.&nbsp; The <b>write</b> method writes 
audio data to the mixer via the source data line. The  bytes 
 
are read from the specified array, starting at the given offset, and are written to the data line's buffer.<p>
<font color="#FF0000"><b>When the input stream is exhausted ...</b></font><p>
When the <b>read</b> method returns -1, indicating that the input data is exhausted, 
control is transferred to the code in Listing 10.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>      sourceDataLine.<b>drain</b>();
      sourceDataLine.<b>close</b>();
    }catch (Exception e) {
      System.out.println(e);
      System.exit(0);
    }//end catch
  }//end run
}//end inner class PlayThread

<b><font face="Courier New,Courier">Listing 10</font></b></pre>
</td>
</tr>
</table>
  <p>
<font color="#FF0000"><b>Block and wait</b></font><p>
The code in Listing 10 invokes the<b> drain </b>method on the <b>SourceDataLine</b> 
object to cause the program to 
block and wait for the internal buffer of the <b>SourceDataLine</b> to become 
empty.&nbsp; When it becomes empty, this means that all of the audio data has been delivered to the speaker.<p>
<font color="#FF0000"><b>Close the SourceDataLine</b></font><p>
Then the code in Listing 10 invokes the <b>close</b> method to close the line, 
indicating that any system resources in use by the line can be released.&nbsp; 
Sun has this to say about closing a line:<blockquote>
  <p>
<i>&quot;Closing a line indicates that any resources used by the line may now be 
released. To free up resources, applications should close lines whenever they 
are not in use, and must close all opened lines when exiting.&nbsp; Mixers are 
assumed to be shared system resources, and can be opened and closed repeatedly. 
Other lines may or may not support re-opening once they have been closed. 
Mechanisms for opening lines vary with the different sub-types and are 
documented where they are defined.&quot;</i></blockquote>
  <p>
<font color="#FF0000"><b>End of story, for now</b></font><p>
So there you have it, an explanation of how this program uses the Java Sound API 
to transfer audio data from an internal memory buffer to an external speaker.<center>
<h2>
<a NAME="Run the program"></a>Run the Program</h2></center>
<p>
      At this point, you may find it useful to compile and run the program in 
      Listing 11 near the end of the lesson.<p>
      <font color="#FF0000"><b>Capture and playback audio data</b></font><p>
      This program demonstrates the ability to capture audio data from a 
      microphone and to play it back through the speakers on your computer.&nbsp; The usage 
      instructions are simple:<ul>
  <li>Start the program running.&nbsp; The simple GUI shown in Figure 6 will appear on the screen.</li>
  <li>Click the <b>Capture</b> button and speak into the microphone.</li>
  <li>Click the <b>Stop</b> button to terminate capturing data.</li>
  <li>Click the <b>Playback</b> button to play your captured voice back through 
  the system speakers.</li>
  </ul>
    <p align="center">
      <img border="0" src="java2008g.gif" width="251" height="71"><p align="center">
      Figure 6 Program GUI<p>
      If you don't hear anything during playback, you may need to increase your speaker volume.<p>
      This program saves the data that it captures in memory, so be careful.&nbsp; 
      If you attempt to save too much data, you may run out of memory.<h2 align="center"><a name="Summary">Summary</a></h2>
<p>I explained that the Java Sound API is based on the concept of <i>lines</i> 
and <i>mixers.</i><p>I provided a description of the physical and electrical 
characteristics of sound, in preparation for the introduction of an<i> audio 
mixer.</i><p>I used the scenario of a rock concert with six microphones and two 
stereo speakers to describe one of the ways that audio mixers are used.<p>I 
discussed a variety of Java Sound programming topics, including mixers, lines, 
data format, etc.<p>I explained the general relationship that exists among <b>
SourceDataLine</b> objects, <b>Clip</b> objects, <b>Mixer</b> objects, <b>
AudioFormat</b> objects, and ports in a simple audio output program.<p>I 
provided a 
program that you can use to first capture and then to play back audio sound.<p>I 
provided a detailed explanation of the code used to play back the audio data 
previously captured in memory by that program.<h2 align="center"><a name="Whats Next">What's Next?</a></h2>
  <p>In this lesson, I explained that the Java Sound API is based on the concept 
  of mixers and lines.&nbsp; However, the audio output code that I explained 
  didn't obviously involve mixers.&nbsp; The<b> AudioSystem</b> class provides 
  static methods that make it possible to write audio programs without having to 
  deal directly with mixers.&nbsp; In other words, the static methods abstract 
  mixers into the background.</p>
  <p>In the next lesson, I will explain the audio capture code in a slightly 
  modified version of the program that was discussed in this lesson.&nbsp; The 
  modified version will make explicit use of mixers in order to show you how you 
  can use them when you need to use them.</p>
<center>
<h2>
<a NAME="Complete Program Listings"></a>Complete Program Listing</h2></center>
A complete listing of the program is shown in Listing 11.<br>
&nbsp;<table BORDER COLS=1 WIDTH="400" BGCOLOR="#FFFF00" >
<tr>
<td>
<pre>/*File AudioCapture01.java
This program demonstrates the capture
and subsequent playback of audio data.

A GUI appears on the screen containing
the following buttons:
Capture
Stop
Playback

Input data from a microphone is
captured and saved in a
ByteArrayOutputStream object when the
user clicks the Capture button.

Data capture stops when the user clicks
the Stop button.

Playback begins when the user clicks
the Playback button.

Tested using SDK 1.4.0 under Win2000
**************************************/

import javax.swing.*;
import java.awt.*;
import java.awt.event.*;
import java.io.*;
import javax.sound.sampled.*;

public class AudioCapture01
                        extends JFrame{

  boolean stopCapture = false;
  ByteArrayOutputStream
                 byteArrayOutputStream;
  AudioFormat audioFormat;
  TargetDataLine targetDataLine;
  AudioInputStream audioInputStream;
  SourceDataLine sourceDataLine;

  public static void main(
                        String args[]){
    new AudioCapture01();
  }//end main

  public AudioCapture01(){//constructor
    final JButton captureBtn =
                new JButton("Capture");
    final JButton stopBtn =
                   new JButton("Stop");
    final JButton playBtn =
               new JButton("Playback");

    captureBtn.setEnabled(true);
    stopBtn.setEnabled(false);
    playBtn.setEnabled(false);

    //Register anonymous listeners
    captureBtn.addActionListener(
      new ActionListener(){
        public void actionPerformed(
                        ActionEvent e){
          captureBtn.setEnabled(false);
          stopBtn.setEnabled(true);
          playBtn.setEnabled(false);
          //Capture input data from the
          // microphone until the Stop
          // button is clicked.
          captureAudio();
        }//end actionPerformed
      }//end ActionListener
    );//end addActionListener()
    getContentPane().add(captureBtn);

    stopBtn.addActionListener(
      new ActionListener(){
        public void actionPerformed(
                        ActionEvent e){
          captureBtn.setEnabled(true);
          stopBtn.setEnabled(false);
          playBtn.setEnabled(true);
          //Terminate the capturing of
          // input data from the
          // microphone.
          stopCapture = true;
        }//end actionPerformed
      }//end ActionListener
    );//end addActionListener()
    getContentPane().add(stopBtn);

    playBtn.addActionListener(
      new ActionListener(){
        public void actionPerformed(
                        ActionEvent e){
          //Play back all of the data
          // that was saved during
          // capture.
          playAudio();
        }//end actionPerformed
      }//end ActionListener
    );//end addActionListener()
    getContentPane().add(playBtn);

    getContentPane().setLayout(
                     new FlowLayout());
    setTitle("Capture/Playback Demo");
    setDefaultCloseOperation(
                        EXIT_ON_CLOSE);
    setSize(250,70);
    setVisible(true);
  }//end constructor

  //This method captures audio input
  // from a microphone and saves it in
  // a ByteArrayOutputStream object.
  private void captureAudio(){
    try{
      //Get everything set up for
      // capture
      audioFormat = getAudioFormat();
      DataLine.Info dataLineInfo =
                new DataLine.Info(
                  TargetDataLine.class,
                   audioFormat);
      targetDataLine = (TargetDataLine)
                   AudioSystem.getLine(
                         dataLineInfo);
      targetDataLine.open(audioFormat);
      targetDataLine.start();

      //Create a thread to capture the
      // microphone data and start it
      // running.  It will run until
      // the Stop button is clicked.
      Thread captureThread =
                new Thread(
                  new CaptureThread());
      captureThread.start();
    } catch (Exception e) {
      System.out.println(e);
      System.exit(0);
    }//end catch
  }//end captureAudio method

  //This method plays back the audio
  // data that has been saved in the
  // ByteArrayOutputStream
  private void playAudio() {
    try{
      //Get everything set up for
      // playback.
      //Get the previously-saved data
      // into a byte array object.
      byte audioData[] =
                 byteArrayOutputStream.
                         toByteArray();
      //Get an input stream on the
      // byte array containing the data
      InputStream byteArrayInputStream
            = new ByteArrayInputStream(
                            audioData);
      AudioFormat audioFormat =
                      getAudioFormat();
      audioInputStream =
        new AudioInputStream(
          byteArrayInputStream,
          audioFormat,
          audioData.length/audioFormat.
                       getFrameSize());
      DataLine.Info dataLineInfo =
                new DataLine.Info(
                  SourceDataLine.class,
                          audioFormat);
      sourceDataLine = (SourceDataLine)
                   AudioSystem.getLine(
                         dataLineInfo);
      sourceDataLine.open(audioFormat);
      sourceDataLine.start();

      //Create a thread to play back
      // the data and start it
      // running.  It will run until
      // all the data has been played
      // back.
      Thread playThread =
          new Thread(new PlayThread());
      playThread.start();
    } catch (Exception e) {
      System.out.println(e);
      System.exit(0);
    }//end catch
  }//end playAudio

  //This method creates and returns an
  // AudioFormat object for a given set
  // of format parameters.  If these
  // parameters don't work well for
  // you, try some of the other
  // allowable parameter values, which
  // are shown in comments following
  // the declarations.
  private AudioFormat getAudioFormat(){
    float sampleRate = 8000.0F;
    //8000,11025,16000,22050,44100
    int sampleSizeInBits = 16;
    //8,16
    int channels = 1;
    //1,2
    boolean signed = true;
    //true,false
    boolean bigEndian = false;
    //true,false
    return new AudioFormat(
                      sampleRate,
                      sampleSizeInBits,
                      channels,
                      signed,
                      bigEndian);
  }//end getAudioFormat
//===================================//

//Inner class to capture data from
// microphone
class CaptureThread extends Thread{
  //An arbitrary-size temporary holding
  // buffer
  byte tempBuffer[] = new byte[10000];
  public void run(){
    byteArrayOutputStream =
           new ByteArrayOutputStream();
    stopCapture = false;
    try{//Loop until stopCapture is set
        // by another thread that
        // services the Stop button.
      while(!stopCapture){
        //Read data from the internal
        // buffer of the data line.
        int cnt = targetDataLine.read(
                    tempBuffer,
                    0,
                    tempBuffer.length);
        if(cnt > 0){
          //Save data in output stream
          // object.
          byteArrayOutputStream.write(
                   tempBuffer, 0, cnt);
        }//end if
      }//end while
      byteArrayOutputStream.close();
    }catch (Exception e) {
      System.out.println(e);
      System.exit(0);
    }//end catch
  }//end run
}//end inner class CaptureThread
//===================================//
//Inner class to play back the data
// that was saved.
class PlayThread extends Thread{
  byte tempBuffer[] = new byte[10000];

  public void run(){
    try{
      int cnt;
      //Keep looping until the input
      // read method returns -1 for
      // empty stream.
      while((cnt = audioInputStream.
        read(tempBuffer, 0,
            tempBuffer.length)) != -1){
        if(cnt > 0){
          //Write data to the internal
          // buffer of the data line
          // where it will be delivered
          // to the speaker.
          sourceDataLine.write(
                   tempBuffer, 0, cnt);
        }//end if
      }//end while
      //Block and wait for internal
      // buffer of the data line to
      // empty.
      sourceDataLine.drain();
      sourceDataLine.close();
    }catch (Exception e) {
      System.out.println(e);
      System.exit(0);
    }//end catch
  }//end run
}//end inner class PlayThread
//===================================//

}//end outer class AudioCapture01.java

<b><font face="Courier New,Courier">Listing 11</font></b></pre>
</td>
</tr>
</table>
<p>
   <p>
   <hr size=3 width="100%" align=center>
<p>Copyright 2003, Richard G. Baldwin.&nbsp; Reproduction in whole or in
part in any form or medium without express written permission from Richard
Baldwin is prohibited. <h4>
<a NAME="About the author"></a>About the author</h4>
<i><a href="mailto:Baldwin@DickBaldwin.com">Richard Baldwin</a> is a college professor (at Austin Community College in Austin, TX) and private consultant whose primary focus is a combination of Java, C#, and XML. In addition to the many platform and/or language independent benefits of Java and C# applications, he believes that a combination of Java, C#, and XML will become the primary driving force in the delivery of structured information on the Web.</i><br><p><i>Richard has participated in numerous consulting projects and he
frequently provides onsite training at the high-tech companies located
in and around Austin, Texas.&nbsp; He is the author of Baldwin's Programming <a href="http://www.DickBaldwin.com">Tutorials</a>,
which has gained a worldwide following among experienced and aspiring programmers.
He has also published articles in JavaPro magazine.</i> <p><i>Richard holds an MSEE degree from Southern Methodist University and
has many years of experience in the application of computer technology
to real-world problems.</i> <p><i><a href="mailto:Baldwin@DickBaldwin.com">Baldwin@DickBaldwin.com</a></i> <p>-end- </body></html>